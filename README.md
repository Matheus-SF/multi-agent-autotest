# AutoTest ‚Äî Gera√ß√£o Autom√°tica de Testes com Multi-Agentes

![Python](https://img.shields.io/badge/Python-3.11-3776AB?style=flat&logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115-009688?style=flat&logo=fastapi&logoColor=white)
![React](https://img.shields.io/badge/React-18-61DAFB?style=flat&logo=react&logoColor=black)
![LangChain](https://img.shields.io/badge/LangChain-0.2-1C3C3C?style=flat&logo=langchain&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-sandbox-2496ED?style=flat&logo=docker&logoColor=white)
![OpenAI](https://img.shields.io/badge/GPT--4o-OpenAI-412991?style=flat&logo=openai&logoColor=white)

Sistema que recebe arquivos Python, executa um pipeline de agentes de IA e entrega testes pytest com cobertura verificada.

> **An English version of this documentation is available at the [end of this page](#-english-version).**

> **TL;DR** Sistema multi-agente que analisa c√≥digo Python, gera testes automaticamente, executa em sandbox isolado e itera at√© atingir um threshold de cobertura configur√°vel. O projeto foca em arquitetura de sistemas com LLMs, loops de avalia√ß√£o autom√°tica e orquestra√ß√£o baseada em grafo.

---

## Como funciona

```mermaid
flowchart TD
    A([Usu√°rio envia arquivos .py<br/>e define threshold]) --> B

    B[Analisador<br/>Mapeia fun√ß√µes, par√¢metros,<br/>types e edge cases]
    B --> C

    C[Escritor<br/>Gera testes pytest<br/>com mocks adequados]
    C --> D

    D[Executor<br/>Roda pytest no sandbox Docker<br/>Coleta coverage.xml e junit.xml]
    D --> E

    E[Revisor<br/>Verifica se cobertura<br/>atingiu o threshold]

    E -->|Cobertura < threshold<br/>e itera√ß√µes dispon√≠veis| C
    E -->|Cobertura >= threshold<br/>ou m√°ximo de itera√ß√µes| F

    F([Relat√≥rio final<br/>Cobertura real ¬∑ Testes passados/falhados<br/>Linhas n√£o cobertas ¬∑ HTML do coverage])

    style A fill:#1d3d5a,stroke:#5aabf0,color:#e2eaf4
    style F fill:#1a3d2a,stroke:#5ec490,color:#e2eaf4
    style B fill:#1e2530,stroke:#334155,color:#e2eaf4
    style C fill:#1e2530,stroke:#334155,color:#e2eaf4
    style D fill:#1e2530,stroke:#334155,color:#e2eaf4
    style E fill:#1e2530,stroke:#334155,color:#e2eaf4
```

---

## Por que esse projeto?

Escrever testes √© uma das partes mais importantes do desenvolvimento. Ferramentas que apenas *sugerem* testes resolvem metade do problema, j√° que voc√™ ainda precisa executar, verificar cobertura e iterar manualmente.

A ideia foi construir um sistema que fecha esse ciclo sozinho: gera, executa, mede e itera at√© atingir um threshold de cobertura configur√°vel. O resultado √© um arquivo de testes funcional e um relat√≥rio HTML de cobertura linha por linha.

### Problema

Ferramentas de gera√ß√£o de testes baseadas em LLM normalmente param na etapa de sugest√£o de c√≥digo. Na pr√°tica, o ciclo real de testes envolve:

1. Executar o c√≥digo gerado
2. Medir cobertura
3. Identificar lacunas
4. Iterar com novas hip√≥teses

Automatizar esse loop √© dif√≠cil porque exige:

- Orquestra√ß√£o de m√∫ltiplas etapas com estado
- Execu√ß√£o segura de c√≥digo n√£o confi√°vel
- Crit√©rios objetivos para parada
- Adapta√ß√£o do comportamento do modelo a cada itera√ß√£o

O projeto explora exatamente esse espa√ßo: um pipeline agentic que fecha o ciclo completo de gera√ß√£o e valida√ß√£o de testes.

---

## Arquitetura

O sistema usa um grafo de agentes orquestrado pelo **LangGraph**, onde cada n√≥ tem uma responsabilidade √∫nica:

```mermaid
flowchart LR
    UI[Frontend] --> AN[Analisador]
    AN --> ES[Escritor]
    ES --> EX[Executor]
    EX --> RE[Revisor]
    RE -->|cobertura < threshold| ES
    RE -->|cobertura atingida| UI

    style UI fill:#1d3d5a,stroke:#5aabf0,color:#e2eaf4
    style AN fill:#1e2530,stroke:#5aabf0,color:#e2eaf4
    style ES fill:#1e2530,stroke:#5aabf0,color:#e2eaf4
    style EX fill:#1e2530,stroke:#e0b84e,color:#e2eaf4
    style RE fill:#1e2530,stroke:#5aabf0,color:#e2eaf4
```

| Agente | Responsabilidade | Temperatura |
|---|---|---|
| Analisador | Mapeia fun√ß√µes, par√¢metros, tipos e edge cases | 0 |
| Escritor | Gera testes pytest com mocks adequados | 0.2 |
| Executor | Roda pytest no Docker, coleta coverage e junit | ‚Äî |
| Revisor | Decide se itera ou encerra baseado no threshold | 0 |

### Stack

| Camada | Tecnologia |
|---|---|
| Orquestra√ß√£o de agentes | LangGraph |
| LLM | GPT-4o via LangChain |
| Backend | FastAPI + Uvicorn |
| Frontend | React + Vite |
| Sandbox de execu√ß√£o | Docker (Python 3.11, pytest, pytest-cov) |
| Prompts | Jinja2 templates |

### Decis√µes de design

**Multi-agentes em vez de um √∫nico agente**
Separar responsabilidades reduz complexidade de prompt, facilita controle de temperatura e torna o comportamento mais previs√≠vel ao longo das itera√ß√µes.

**Loop baseado em cobertura**
Cobertura √© uma m√©trica objetiva e f√°cil de automatizar. N√£o mede corretude, mas funciona bem como crit√©rio de progresso incremental.

**Execu√ß√£o em sandbox Docker**
Permite rodar c√≥digo potencialmente inseguro sem comprometer o ambiente do host e garante reprodutibilidade.

**Prompts como templates versionados**
Manter prompts em Jinja2 desacopla l√≥gica de orquestra√ß√£o do comportamento do modelo, facilitando ajustes finos sem alterar c√≥digo Python.

**LangGraph em vez de chain linear**
O fluxo possui bifurca√ß√£o e itera√ß√£o condicional. Um grafo representa melhor esse tipo de controle do que pipelines sequenciais.

---

## O que foi constru√≠do

- Pipeline multi-agente com loop condicional real, n√£o √© uma chain linear
- Sandbox Docker isolado para execu√ß√£o segura de c√≥digo de terceiros
- Prompts separados em Jinja2 com l√≥gica condicional por itera√ß√£o
- Backend FastAPI com endpoint de upload multipart e serving de arquivos est√°ticos
- Frontend React com feedback visual de etapa atual, ring de cobertura SVG e link direto para o HTML do coverage
- Corre√ß√£o autom√°tica de imports e padr√µes problem√°ticos de mock nos testes gerados
- Relat√≥rio final com cobertura, testes passados/falhados e linhas n√£o cobertas por arquivo

### O que este projeto demonstra

- Modelagem de sistemas multi-agente com responsabilidades bem definidas
- Orquestra√ß√£o de loops de avalia√ß√£o autom√°tica com LLMs
- Integra√ß√£o de modelos com ferramentas reais de engenharia (pytest, coverage, Docker)
- Design de prompts orientado a comportamento e itera√ß√£o
- Constru√ß√£o de pipelines determin√≠sticos envolvendo execu√ß√£o de c√≥digo
- Capacidade de traduzir um problema de engenharia em arquitetura execut√°vel

---

## Resultados

| Arquivo | Cobertura | Itera√ß√µes |
|---|---|---|
| `sample_code.py` (algoritmos cl√°ssicos) | 100% | 2 |
| `library.py` (sistema de biblioteca) | 96% | 1 |
| `executor.py` (infraestrutura com Docker) | 49% | 5 |

![Resultado sample_code](multi-agent-autotest/assets/result_sample_code.png)
![Resultado library](multi-agent-autotest/assets/result_library.png)

---

## Aprendizados

**LangGraph na pr√°tica,** a diferen√ßa entre um grafo e uma chain simples √© o loop condicional. Implementar o `should_continue` que decide entre iterar ou encerrar foi o ponto central da arquitetura.

**Prompts como c√≥digo,** separar os prompts em arquivos Jinja2 foi uma decis√£o que pagou dividendos. Ajustar o comportamento do Escritor nas itera√ß√µes subsequentes (focar s√≥ nas linhas n√£o cobertas) sem tocar no Python foi simples e r√°pido.

**Sandboxing tem nuances,** a primeira vers√£o montava o diret√≥rio de c√≥digo como read-only (`:ro`), o que quebrava o pytest-cov que precisa escrever o `.coverage`. O `PYTHONPATH=/code` foi outro ajuste necess√°rio para os testes conseguirem importar o c√≥digo sendo testado.

**LLMs geram mocks problem√°ticos,** o padr√£o `side_effect=lambda p: ...` gerado pelo GPT-4o quebra quando `Path.exists` √© chamado internamente pelo pytest com argumentos extras. A solu√ß√£o foi uma fun√ß√£o de p√≥s-processamento que corrige automaticamente esses padr√µes antes de salvar os testes.

**Testes passados n√£o s√£o cobertura,** 100% de cobertura com testes falhando √© poss√≠vel e aconteceu. Cobertura mede linhas executadas, n√£o corretude das assertivas. O sistema mostra os dois separadamente por esse motivo.

---

## Limita√ß√µes conhecidas

**Depend√™ncias externas,** o sandbox tem apenas `pytest` e `pytest-cov`. C√≥digo que importa bibliotecas de terceiros precisar√° de mocks, o que pode reduzir a qualidade dos testes gerados.

**C√≥digo de infraestrutura,** arquivos que dependem de Docker, subprocess ou filesystem real s√£o dif√≠ceis de testar com mocks. Nesses casos a cobertura ser√° parcial.

**Casos de uso ideais,** c√≥digo Python puro com l√≥gica de neg√≥cio, algoritmos, parsers e m√≥dulos sem depend√™ncias externas pesadas.

### Escopo e n√£o-objetivos

Este projeto foi desenvolvido como demonstra√ß√£o de arquitetura e conceitos de AI Engineering. N√£o √© objetivo:

- Otimiza√ß√£o de custo ou lat√™ncia
- Suporte completo a todos os tipos de c√≥digo Python
- Avalia√ß√£o sem√¢ntica profunda da qualidade dos testes
- Persist√™ncia de estado ou mem√≥ria de longo prazo
- Hardening completo de seguran√ßa do sandbox

O foco est√° em demonstrar o desenho do sistema e o funcionamento do loop agentic.

---

## Melhorias poss√≠veis

- **Isolamento completo do sandbox,** `--network none` e usu√°rio sem privil√©gios dentro do container
- **Suporte a `requirements.txt`,** instalar depend√™ncias do usu√°rio no sandbox antes de executar
- **Diagn√≥stico de falhas,** quando um teste falha, um agente adicional analisa o motivo e decide se √© bug no teste ou bug no c√≥digo
- **Suporte a outras linguagens,** JavaScript/TypeScript com Jest seria o pr√≥ximo passo natural

---

## Como rodar localmente

### Pr√©-requisitos
- Python 3.11
- Node.js 18+
- Docker
- Chave de API da OpenAI

### Instala√ß√£o

```bash
git clone https://github.com/Matheus-SF/multi-agent-autotest.git
cd multi-agent-autotest
```

### Backend
```bash
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env
# Adicione sua OPENAI_API_KEY no .env
```

### Sandbox
```bash
docker build -t autotest-sandbox ./sandbox
```

### Frontend
```bash
cd frontend
npm install
```

### Inicializar
```bash
# Terminal 1
make backend

# Terminal 2
make frontend
```

Acesse `http://localhost:5173`.

---

## Estrutura do projeto

```
multi-agent-autotest/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py      # Agente Analisador
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ writer.py        # Agente Escritor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reviewer.py      # Agente Revisor
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ graph.py         # Grafo LangGraph
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ executor.py      # Execu√ß√£o no sandbox Docker
‚îÇ   ‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyzer.j2
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ writer.j2
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reviewer.j2
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îú‚îÄ‚îÄ main.py
‚îÇ       ‚îî‚îÄ‚îÄ routes.py
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îî‚îÄ‚îÄ App.jsx
‚îú‚îÄ‚îÄ sandbox/
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ sample_code.py       # Algoritmos cl√°ssicos
‚îÇ   ‚îî‚îÄ‚îÄ library.py           # Sistema de biblioteca
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ result_sample_code.png
‚îÇ   ‚îî‚îÄ‚îÄ result_library.png
‚îî‚îÄ‚îÄ Makefile
```

---

## üá∫üá∏ English version

# AutoTest ‚Äî Automated Test Generation with Multi-Agents

System that receives Python files, runs a multi-agent AI pipeline and delivers pytest tests with verified coverage.

> **TL;DR** Multi-agent system that analyzes Python code, generates tests automatically, executes them in an isolated sandbox and iterates until a configurable coverage threshold is reached. The project focuses on LLM-based system architecture, automated evaluation loops and graph-based orchestration.

---

### How it works

The user uploads `.py` files and sets a coverage threshold. The system then runs a pipeline of four agents, each with a single responsibility:

1. **Analyzer** maps functions, parameters, types and edge cases
2. **Writer** generates pytest tests with appropriate mocks
3. **Executor** runs pytest inside an isolated Docker sandbox and collects real coverage data
4. **Reviewer** checks whether the threshold was met and decides to iterate or stop

The loop continues until the target coverage is reached or the maximum number of iterations is exhausted.

---

### Why this project?

LLM-based test generation tools usually stop at the suggestion step. In practice, the real testing cycle involves executing the generated code, measuring coverage, identifying gaps and iterating with new hypotheses.

Automating this loop requires orchestration of multiple stateful steps, safe execution of untrusted code, objective stopping criteria and adaptive model behavior across iterations.

This project explores exactly that space: an agentic pipeline that closes the complete cycle of test generation and validation.

---

### Architecture

The system uses a LangGraph agent graph where each node has a single responsibility.

| Agent | Responsibility | Temperature |
|---|---|---|
| Analyzer | Maps functions, parameters, types and edge cases | 0 |
| Writer | Generates pytest tests with appropriate mocks | 0.2 |
| Executor | Runs pytest in Docker, collects coverage and junit | ‚Äî |
| Reviewer | Decides whether to iterate or stop based on threshold | 0 |

**Stack:** LangGraph, GPT-4o via LangChain, FastAPI, React + Vite, Docker sandbox, Jinja2 prompts.

---

### Results

| File | Coverage | Iterations |
|---|---|---|
| `sample_code.py` (classic algorithms) | 100% | 2 |
| `library.py` (library management system) | 96% | 1 |
| `executor.py` (Docker infrastructure) | 49% | 5 |

---

### What this project demonstrates

- Multi-agent system design with well-defined responsibilities
- Automated evaluation loops for LLM outputs
- Integration of models with real engineering tools (pytest, coverage, Docker)
- Prompt engineering oriented toward behavior and iteration
- Building deterministic pipelines involving code execution

---

### Known limitations

- The sandbox only has `pytest` and `pytest-cov` installed. Code that imports third-party libraries will require mocks, which may reduce test quality.
- Infrastructure code that depends on Docker, subprocess or the real filesystem is difficult to test with mocks.
- The LLM may generate tests that cover lines but with incorrect assertions. Always review generated tests before using them in production.

---

### Possible improvements

- Full sandbox isolation with `--network none` and unprivileged user
- Support for `requirements.txt` to install user dependencies before execution
- Failure diagnosis agent that distinguishes bugs in tests from bugs in code
- Support for other languages, JavaScript/TypeScript with Jest would be the natural next step
